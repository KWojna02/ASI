# Predictive Maintenance System - Architecture Diagrams

## 1. High-Level System Architecture

```mermaid
graph TB
    subgraph "Data Sources"
        S1[Sensor Simulator]
        S2[IoT Devices]
        S3[SCADA Systems]
    end
    
    subgraph "Ingestion Layer"
        K[Apache Kafka Cluster]
        SR[Schema Registry]
    end
    
    subgraph "Stream Processing"
        KS[Kafka Streams]
        SP[Spark Streaming]
        FE[Feature Engineering]
    end
    
    subgraph "Storage Layer"
        IDB[(InfluxDB<br/>Time-Series)]
        PG[(PostgreSQL<br/>Metadata)]
        S3D[(Object Storage<br/>S3/MinIO)]
        FS[Feature Store<br/>Feast]
    end
    
    subgraph "ML Pipeline"
        TR[Training Pipeline<br/>Spark MLlib]
        ML[Model Training<br/>TensorFlow/PyTorch]
        MR[Model Registry<br/>MLflow]
        EXP[Experiment Tracking]
    end
    
    subgraph "Inference Layer"
        MS[Model Serving<br/>TF Serving]
        API[REST API<br/>FastAPI]
    end
    
    subgraph "Application Layer"
        DASH[Real-time Dashboard<br/>Grafana]
        ALERT[Alert System]
        CMMS[CMMS Integration]
    end
    
    S1 & S2 & S3 --> K
    K --> SR
    K --> KS
    K --> SP
    KS & SP --> FE
    FE --> IDB
    FE --> FS
    IDB --> PG
    IDB --> S3D
    IDB --> TR
    TR --> ML
    ML --> EXP
    ML --> MR
    MR --> MS
    FS --> MS
    MS --> API
    API --> DASH
    API --> ALERT
    ALERT --> CMMS
    
    style K fill:#ff6b6b
    style IDB fill:#4ecdc4
    style ML fill:#95e1d3
    style MS fill:#f38181
    style DASH fill:#ffd93d
```

## 2. Data Flow - Real-time Prediction

```mermaid
sequenceDiagram
    participant Sensor
    participant Kafka
    participant StreamProc as Stream Processing
    participant InfluxDB
    participant FeatureStore
    participant ModelServing
    participant AlertSystem
    participant Dashboard
    
    Sensor->>Kafka: Publish sensor data
    Kafka->>StreamProc: Consume messages
    StreamProc->>StreamProc: Feature engineering
    StreamProc->>InfluxDB: Store raw + features
    StreamProc->>FeatureStore: Update feature cache
    
    Note over StreamProc,FeatureStore: Real-time features
    
    FeatureStore->>ModelServing: Fetch features
    ModelServing->>ModelServing: Run inference
    ModelServing->>Kafka: Publish prediction
    
    alt High Risk Detected
        ModelServing->>AlertSystem: Trigger alert
        AlertSystem->>Dashboard: Update UI
        AlertSystem->>AlertSystem: Send notifications
    end
    
    Kafka->>Dashboard: Stream predictions
    Dashboard->>Dashboard: Update visualization
```

## 3. ML Training Pipeline

```mermaid
flowchart TD
    A[Historical Data<br/>InfluxDB] --> B[Data Extraction<br/>Apache Spark]
    B --> C[Feature Engineering<br/>Window Aggregations]
    C --> D[Feature Store<br/>Write Features]
    D --> E[Train/Val/Test Split<br/>80/10/10]
    E --> F[Model Training<br/>TensorFlow/PyTorch]
    F --> G{Model Validation}
    G -->|Metrics OK| H[Experiment Tracking<br/>MLflow]
    G -->|Failed| I[Alert Team]
    H --> J[Model Registry<br/>Staging]
    J --> K{Business Rules<br/>Validation}
    K -->|Pass| L[A/B Testing<br/>Deployment]
    K -->|Fail| M[Reject Model]
    L --> N[Production<br/>Deployment]
    N --> O[Monitor Performance]
    O -->|Drift Detected| P[Retrain Trigger]
    P --> B
    
    style F fill:#95e1d3
    style J fill:#ffd93d
    style N fill:#f38181
```

## 4. Feature Engineering Pipeline

```mermaid
flowchart LR
    subgraph "Raw Sensors"
        V1[Vibration X/Y/Z]
        T1[Temperature]
        P1[Pressure]
        R1[RPM]
    end
    
    subgraph "Statistical Features"
        S1[Rolling Mean<br/>1h, 6h, 24h]
        S2[Rolling Std<br/>1h, 6h, 24h]
        S3[Rolling Min/Max]
        S4[Percentiles]
    end
    
    subgraph "Temporal Features"
        TF1[Rate of Change]
        TF2[Acceleration]
        TF3[Time Since Start]
        TF4[Running Hours]
    end
    
    subgraph "Frequency Features"
        FF1[FFT Components]
        FF2[Dominant Frequency]
        FF3[Spectral Energy]
        FF4[Harmonic Ratio]
    end
    
    subgraph "Contextual Features"
        CF1[Deviation from Baseline]
        CF2[Cross-sensor Correlation]
        CF3[Operating Mode]
        CF4[Maintenance History]
    end
    
    V1 & T1 & P1 & R1 --> S1 & S2 & S3 & S4
    V1 & T1 & P1 & R1 --> TF1 & TF2 & TF3 & TF4
    V1 --> FF1 & FF2 & FF3 & FF4
    V1 & T1 & P1 & R1 --> CF1 & CF2 & CF3 & CF4
    
    S1 & S2 & S3 & S4 --> FS[Feature Store]
    TF1 & TF2 & TF3 & TF4 --> FS
    FF1 & FF2 & FF3 & FF4 --> FS
    CF1 & CF2 & CF3 & CF4 --> FS
```

## 5. Model Architecture - LSTM Network

```mermaid
flowchart TD
    A[Input Sequence<br/>100 timesteps x N features] --> B[LSTM Layer 1<br/>128 units]
    B --> C[Dropout<br/>0.3]
    C --> D[LSTM Layer 2<br/>64 units]
    D --> E[Dropout<br/>0.3]
    E --> F[Dense Layer<br/>32 units, ReLU]
    F --> G[Dropout<br/>0.2]
    G --> H[Output Layer<br/>1 unit]
    H --> I{Output Type}
    I -->|Regression| J[RUL Hours]
    I -->|Classification| K[Failure Probability]
    
    style B fill:#95e1d3
    style D fill:#95e1d3
    style F fill:#ffd93d
    style H fill:#f38181
```

## 6. Alert Decision Flow

```mermaid
flowchart TD
    A[Model Prediction<br/>Received] --> B{Failure Probability<br/>> 0.8}
    B -->|Yes| C[CRITICAL Alert]
    B -->|No| D{Failure Probability<br/>> 0.6}
    D -->|Yes| E[HIGH Alert]
    D -->|No| F{Failure Probability<br/>> 0.4}
    F -->|Yes| G[MEDIUM Alert]
    F -->|No| H{Anomaly Detected?}
    H -->|Yes| I[LOW Alert]
    H -->|No| J[No Action]
    
    C --> K[Stop Production<br/>Immediate Action]
    E --> L[Schedule Urgent<br/>Maintenance < 24h]
    G --> M[Plan Maintenance<br/>Within 1 week]
    I --> N[Monitor Closely<br/>Increase Sampling]
    
    K --> O[Notify: Operations,<br/>Maintenance, Management]
    L --> P[Notify: Maintenance,<br/>Operations]
    M --> Q[Notify: Maintenance]
    N --> R[Log to Dashboard]
    
    style C fill:#ff6b6b
    style E fill:#ffa07a
    style G fill:#ffd93d
    style I fill:#95e1d3
```

## 7. System Monitoring Architecture

```mermaid
flowchart TB
    subgraph "Data Pipeline Monitoring"
        KM[Kafka Metrics<br/>Lag, Throughput]
        SM[Spark Metrics<br/>Processing Time]
        DM[Data Quality<br/>Checks]
    end
    
    subgraph "Model Monitoring"
        PM[Prediction Metrics<br/>Latency, Throughput]
        DD[Data Drift<br/>Detection]
        PD[Prediction Drift<br/>Detection]
        MM[Model Performance<br/>Online Metrics]
    end
    
    subgraph "Infrastructure Monitoring"
        IM[Resource Usage<br/>CPU, Memory, Disk]
        NM[Network Metrics<br/>Bandwidth, Latency]
        AM[Application Logs<br/>Errors, Warnings]
    end
    
    subgraph "Observability Stack"
        PROM[Prometheus<br/>Metrics Collection]
        GRAF[Grafana<br/>Visualization]
        ALERT[AlertManager<br/>Notifications]
        LOKI[Loki<br/>Log Aggregation]
    end
    
    KM & SM & DM --> PROM
    PM & DD & PD & MM --> PROM
    IM & NM --> PROM
    AM --> LOKI
    
    PROM --> GRAF
    PROM --> ALERT
    LOKI --> GRAF
    
    style PROM fill:#ff6b6b
    style GRAF fill:#4ecdc4
    style ALERT fill:#ffd93d
```

## 8. Deployment Architecture

```mermaid
flowchart TB
    subgraph "Development"
        DEV1[Data Scientists]
        DEV2[ML Engineers]
        DEV3[Feature Development]
    end
    
    subgraph "CI/CD Pipeline"
        CI1[Code Commit<br/>Git]
        CI2[Automated Tests<br/>Unit, Integration]
        CI3[Model Validation<br/>Metrics Check]
        CI4[Container Build<br/>Docker]
        CI5[Deploy to Staging]
    end
    
    subgraph "Staging Environment"
        ST1[Shadow Deployment]
        ST2[A/B Testing<br/>10% Traffic]
        ST3[Performance<br/>Monitoring]
    end
    
    subgraph "Production Environment"
        PROD1[Model Serving<br/>Cluster]
        PROD2[Load Balancer]
        PROD3[Auto-scaling]
        PROD4[Monitoring &<br/>Alerting]
    end
    
    DEV1 & DEV2 --> DEV3
    DEV3 --> CI1
    CI1 --> CI2
    CI2 --> CI3
    CI3 --> CI4
    CI4 --> CI5
    CI5 --> ST1
    ST1 --> ST2
    ST2 --> ST3
    ST3 -->|Approved| PROD1
    ST3 -->|Rejected| DEV3
    PROD1 --> PROD2
    PROD2 --> PROD3
    PROD3 --> PROD4
    PROD4 -->|Issues| DEV3
    
    style CI3 fill:#95e1d3
    style ST2 fill:#ffd93d
    style PROD1 fill:#f38181
```

## 9. Data Storage Strategy

```mermaid
flowchart LR
    subgraph "Hot Storage"
        H1[InfluxDB<br/>Last 90 days<br/>Full resolution]
    end
    
    subgraph "Warm Storage"
        W1[InfluxDB<br/>91-365 days<br/>5-min aggregates]
    end
    
    subgraph "Cold Storage"
        C1[S3/MinIO<br/>1-5 years<br/>1-hour aggregates]
        C2[Compressed<br/>Parquet Files]
    end
    
    subgraph "Archive"
        A1[Glacier<br/>> 5 years<br/>Compliance/Audit]
    end
    
    H1 -->|Downsample| W1
    W1 -->|Downsample| C1
    C1 --> C2
    C2 -->|Archive| A1
    
    H1 -.->|Query Time<br/>< 1s| Q1[Real-time<br/>Queries]
    W1 -.->|Query Time<br/>< 5s| Q2[Analytics<br/>Queries]
    C1 -.->|Query Time<br/>< 30s| Q3[Historical<br/>Analysis]
    
    style H1 fill:#ff6b6b
    style W1 fill:#ffd93d
    style C1 fill:#4ecdc4
    style A1 fill:#95e1d3
```

## 10. Scalability Architecture

```mermaid
flowchart TB
    subgraph "Horizontal Scaling"
        HS1[Kafka Brokers<br/>Add partitions]
        HS2[Spark Executors<br/>Add nodes]
        HS3[Model Serving<br/>Add containers]
        HS4[InfluxDB<br/>Clustering]
    end
    
    subgraph "Load Balancing"
        LB1[Kafka: Partition<br/>Assignment]
        LB2[Spark: Task<br/>Distribution]
        LB3[Model: Round-robin<br/>+ Health Checks]
    end
    
    subgraph "Caching"
        C1[Feature Cache<br/>Redis]
        C2[Model Cache<br/>In-memory]
        C3[Query Cache<br/>InfluxDB]
    end
    
    subgraph "Auto-scaling Rules"
        AS1[CPU > 70%:<br/>Scale up]
        AS2[Kafka Lag > 1000:<br/>Add consumers]
        AS3[Inference latency > 100ms:<br/>Add servers]
    end
    
    HS1 --> LB1
    HS2 --> LB2
    HS3 --> LB3
    
    LB1 & LB2 & LB3 --> C1 & C2 & C3
    
    AS1 & AS2 & AS3 --> HS1 & HS2 & HS3
    
    style HS1 fill:#95e1d3
    style HS2 fill:#95e1d3
    style HS3 fill:#95e1d3
    style C1 fill:#ffd93d
```

---

## Legend

- **Red (#ff6b6b)**: Critical components requiring high availability
- **Teal (#4ecdc4)**: Storage and data management
- **Light Green (#95e1d3)**: Machine learning and processing
- **Light Red (#f38181)**: Model serving and inference
- **Yellow (#ffd93d)**: Monitoring and observability

---

**Note**: These diagrams can be rendered using any Mermaid-compatible viewer or integrated into documentation platforms like GitLab, GitHub, Confluence, or Notion.
